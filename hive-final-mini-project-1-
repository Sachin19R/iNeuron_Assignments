This is a real time dataset of the ineuron technical consultant team. You have to perform hive analysis on this given dataset.

Download Dataset 1 - https://drive.google.com/file/d/1WrG-9qv6atP-W3P_-gYln1hHyFKRKMHP/view

Download Dataset 2 - https://drive.google.com/file/d/1-JIPCZ34dyN6k9CqJa-Y8yxIGq6vTVXU/view

Note: both files are csv files. 


1. Create a schema based on the given dataset

a). Schema for AgentLogingReport

hive> create table Agent_Loging_Report
    > (
    > Sr_No int,
    > Agent string,
    > Date string,
    > Login_Time string,
    > Logout_Time string,
    > Duration string
    > )
    > row format delimited
    > fields terminated by ','
    > tblproperties("skip.header.line.count" = "1");
OK
Time taken: 0.113 seconds

b). Schema for AgentPerformance

hive> create table Agent_Performance
    > (
    > Sr_NO int,
    > Date string,
    > Agent_Name string,
    > Total_Chats int,
    > Avg_Response_Time string,
    > Avg_Resolution_Time String,
    > Avg_Rating float,
    > Total_Feedback int
    > )
    > row format delimited
    > fields terminated by ','
    > tblproperties("skip.header.line.count" = "1")
    > ;
OK
Time taken: 0.796 seconds


2. Dump the data inside the hdfs in the given schema location.

a). Loading data into agent_loging_report table

hive> load data local inpath 'data/AgentLogingReport.csv' into table Agent_Loging_Report;
Loading data to table hive_project.agent_loging_report
Table hive_project.agent_loging_report stats: [numFiles=1, totalSize=55351]
OK
Time taken: 0.773 seconds

b). Loading data into agent_loging_report table

hive> load data local inpath 'data/AgentPerformance.csv' into table Agent_Performance;
Loading data to table hive_project.agent_performance
Table hive_project.agent_performance stats: [numFiles=1, totalSize=109853]
OK
Time taken: 0.504 seconds


3. List of all agents' names. 

hive> select distinct(agent_name) from agent_performance;


4. Find out agent average rating.

hive> select agent_name,avg(avg_rating) as Overall_rating
    > from agent_performance
    > group by agent_name;

5. Total working days for each agents 

hive> select x.agent,sum(x.date) as total_woring_days
    > from (
    > select agent,count(distinct(date)) as date
    > from agent_loging_report
    > group by agent,date) as x
    > group by x.agent;

6. Total query that each agent have taken 

hive> select agent_name,sum(total_chats) as total_queries_taken
    > from agent_performance
    > group by agent_name;


7. Total Feedback that each agent have received 

hive> select agent_name,sum(total_feedback) as total_feedback
    > from agent_performance
    > group by agent_name;

8. Agent name who have average rating between 3.5 to 4 

hive> select x.agent_name
    > from (
    > select agent_name,avg(avg_rating) as Overall_rating
    > from agent_performance
    > group by agent_name) as x
    > where x.overall_rating between 3.5 and 4;

9. Agent name who have rating less than 3.5 

hive> select x.agent_name
    > from (
    > select agent_name,avg(avg_rating) as Overall_rating
    > from agent_performance
    > group by agent_name) as x
    > where x.overall_rating < 3.5;

10. Agent name who have rating more than 4.5 

hive> select x.agent_name
    > from (
    > select agent_name,avg(avg_rating) as Overall_rating
    > from agent_performance
    > group by agent_name) as x
    > where x.overall_rating > 4.5;

11. How many feedback agents have received more than 4.5 average

hive> select agent_name , sum(total_feedback) as total_feedback
    > from agent_performance
    > where avg_rating > 4.5
    > group by agent_name;

12. average weekly response time for each agent 

#the data in the date column is not peristent and also not in std YYYY-MM-DD format so to clean it i am using PYTHON UDF.

[cloudera@quickstart ~]$ cat data/date_transform.py
import sys

for line in sys.stdin:
    line = line.strip("\n\r")
    date, name , response_time,resolution_time=line.split('\t')
    mm, dd, yy = date.split('/')
    if len(mm) == 1:
        mm='0'+mm
    if len(dd) == 1:
        dd='0'+dd
    date1='-'.join([yy,mm,dd])
    result='\t'.join([date1,name,response_time,resolution_time])
    print(result)
    
    
#Adding pyhton file in hive
   
hive> add file data/date_transform.py;
Added resources: [data/date_transform.py]

#creating new transformed table

hive> create table agent_performance_transformed as
    > select transform(date,agent_name,avg_response_time,avg_resolution_time)
    > using 'python date_transform.py'
    > as (date date,agent_name string,avg_response_time string,avg_resolution_time string)
    > from agent_performance;
    
#average weekly response time for each agent

hive> with cte as
    > (
    > select date , agent_name , avg_response_time ,
    > (hour(avg_response_time)*3600)+(minute(avg_response_time)*60)+(second(avg_response_time)) as time_in_secs ,
    > case when day(date) < 8 then 'WEEK_1'
    > when day(date) < 15 then 'WEEK_2'
    > when day(date) < 22 then 'WEEK_3'
    > when day(date) < 29 then 'WEEK_4'
    > when day(date) > 28 then 'WEEK_5'
    > end as week
    > from agent_performance_transformed
    > )
    >
    > select x.agent_name,x.week,
    > concat(cast(seconds/60 as int),' MIN ' , seconds%60,' SECs') as weekly_avg_response_time
    > from
    > (
    > select cte.agent_name,cte.week,
    > cast(avg(cte.time_in_secs) as int) as seconds
    > from cte
    > group by cte.agent_name,cte.week
    > ) as x;

#sample output---->

x.agent_name    x.week  weekly_avg_response_time
Aditya Shinde   WEEK_1  0 MIN 54 SECs
Aditya Shinde   WEEK_2  0 MIN 53 SECs
Aditya Shinde   WEEK_3  0 MIN 19 SECs
Aditya Shinde   WEEK_4  0 MIN 0 SECs
Aditya Shinde   WEEK_5  0 MIN 0 SECs



13. average weekly resolution time for each agents 

hive> with cte as
    > (
    > select date , agent_name , avg_resolution_time ,
    > (hour(avg_resolution_time)*3600)+(minute(avg_resolution_time)*60)+(second(avg_resolution_time)) as time_in_secs ,
    > case when day(date) < 8 then 'WEEK_1'
    > when day(date) < 15 then 'WEEK_2'
    > when day(date) < 22 then 'WEEK_3'
    > when day(date) < 29 then 'WEEK_4'
    > when day(date) > 28 then 'WEEK_5'
    > end as week
    > from agent_performance_transformed
    > )
    >
    > select x.agent_name,x.week,
    > concat(cast(seconds/60 as int),' MIN ' , seconds%60,' SECs') as weekly_avg_resolution_time
    > from
    > (
    > select cte.agent_name,cte.week,
    > cast(avg(cte.time_in_secs) as int) as seconds
    > from cte
    > group by cte.agent_name,cte.week
    > ) as x;

#sample output

x.agent_name    x.week  weekly_avg_resolution_time
Aditya Shinde   WEEK_1  15 MIN 38 SECs
Aditya Shinde   WEEK_2  18 MIN 30 SECs
Aditya Shinde   WEEK_3  10 MIN 10 SECs
Aditya Shinde   WEEK_4  0 MIN 0 SECs
Aditya Shinde   WEEK_5  0 MIN 0 SECs
Aditya_iot      WEEK_1  3 MIN 37 SECs
Aditya_iot      WEEK_2  13 MIN 11 SECs
Aditya_iot      WEEK_3  11 MIN 40 SECs
Aditya_iot      WEEK_4  11 MIN 42 SECs
Aditya_iot      WEEK_5  6 MIN 41 SECs


14. Find the number of chat on which they have received a feedback

hive> select agent_name,
    > sum(total_chats) as queries_attended,
    > sum(total_feedback) as feedback_recived
    > from agent_performance
    > group by agent_name;

15. Total contribution hour for each and every agents weekly basisT 

hive> select t1.agent,t1.week,
    > concat(sum(hour(t1.duration)) + cast(sum(minute(t1.duration))/60 as int),
    > ' Hrs ', sum(minute(t1.duration))%60, ' MIN') as clocked_hours
    > from
    > (
    > select agent , date, duration,
    > case when substr(date,0,2) < 8 then 'WEEK_1'
    > when substr(date,0,2) < 15 then 'WEEK_2'
    > when substr(date,0,2) < 22 then 'WEEK_3'
    > when substr(date,0,2) < 29 then 'WEEK_4'
    > when substr(date,0,2) > 28 then 'WEEK_5'
    > end as week
    > from agent_loging_report
    > ) as t1
    > group by t1.agent,t1.week;


#SAMPLE O/P

t1.agent        t1.week clocked_hours
Aditya Shinde   WEEK_4  0 Hrs 2 MIN
Aditya_iot      WEEK_3  4 Hrs 7 MIN
Aditya_iot      WEEK_4  9 Hrs 20 MIN
Aditya_iot      WEEK_5  2 Hrs 12 MIN
Amersh  WEEK_4  3 Hrs 1 MIN
Ameya Jain      WEEK_3  11 Hrs 12 MIN
Ameya Jain      WEEK_4  30 Hrs 47 MIN
Ankitjha        WEEK_4  2 Hrs 14 MIN


16. Perform inner join, left join and right join based on the agent column and after joining the table export that data into your local system.
17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.
